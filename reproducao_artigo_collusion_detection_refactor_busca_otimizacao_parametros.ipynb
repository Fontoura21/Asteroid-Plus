{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fontoura21/Asteroid-Plus/blob/main/reproducao_artigo_collusion_detection_refactor_busca_otimizacao_parametros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8-CPsiugEbD",
        "outputId": "2fd17cb4-bdd2-435e-ce77-0504d57edd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (23.9.7)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlBOVG-SHxoo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from joblib import dump, load\n",
        "from matplotlib.lines import Line2D\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from collections import defaultdict\n",
        "from scipy import stats\n",
        "from itertools import cycle\n",
        "from sklearn.utils import shuffle\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls627hkQH1eC"
      },
      "outputs": [],
      "source": [
        "# Font size to plot\n",
        "default_font_size = 18\n",
        "plt.rcParams.update({'font.size': default_font_size})\n",
        "\n",
        "# Format to print\n",
        "pd.options.display.float_format = '{:,.4f}'.format\n",
        "\n",
        "# To hide warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Paths and filenames of the datasets\n",
        "path = '../content/drive/MyDrive/Céos/Artigos e Documentos'\n",
        "db_collusion_brazilian = os.path.join(path, 'DB_Collusion_Brazil_processed.csv')\n",
        "db_collusion_italian = os.path.join(path, 'DB_Collusion_Italy_processed.csv')\n",
        "db_collusion_american = os.path.join(path, 'DB_Collusion_America_processed.csv')\n",
        "db_collusion_switzerland_gr_sg = os.path.join(path, 'DB_Collusion_Switzerland_GR_and_See-Gaster_processed.csv')\n",
        "db_collusion_switzerland_ticino = os.path.join(path, 'DB_Collusion_Switzerland_Ticino_processed.csv')\n",
        "db_collusion_japan = os.path.join(path, 'DB_Collusion_Japan_processed.csv')\n",
        "db_collusion_all = os.path.join(path, 'DB_Collusion_All_processed.csv')\n",
        "\n",
        "# To save plots (pdf format)\n",
        "plot_pdf = True\n",
        "\n",
        "# User's parameters for the functions\n",
        "#ml_algorithms = ['GaussianProcessClassifier', 'SGDClassifier', 'ExtraTreesClassifier', 'RandomForestClassifier', 'AdaBoostClassifier',\n",
        "#                  'GradientBoostingClassifier', 'SVC', 'KNeighborsClassifier', 'MLPClassifier', 'BernoulliNB', 'GaussianNB', 'LogisticRegression']\n",
        "#ml_algorithms = ['GradientBoostingClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier']\n",
        "#ml_algorithms = ['LogisticRegression']\n",
        "#ml_algorithms = ['MLPClassifier']\n",
        "ml_algorithms = ['SVC']\n",
        "screens = ['CV', 'SPD', 'DIFFP', 'RD', 'KURT', 'SKEW', 'KSTEST'] # Screening variables to use. There are seven: CV, SPD, DIFFP, RD, KURT, SKEW and KSTEST\n",
        "train_size = 0.8 # Test and train sizes. The test_size is 1-train_size\n",
        "repetitions = 40 # Number of repetitions for each ML algorithm. Minimum value > 30. Recommended value > 100\n",
        "n_estimators = 300 # Number of estimators for ML algorithms\n",
        "precision_recall = False # To plot precision-recall curves\n",
        "load_data = False # To load the error metrics (to load previous data experimentation)\n",
        "save_data = True # To save the error metrics (to persist the data experimentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q10LbO-_cKk",
        "outputId": "6891c253-8603-42cd-d994-6b13756e5f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-CUc6ExGGCq"
      },
      "outputs": [],
      "source": [
        "def predict_collusion_company(df, dataset, predictors_column_name, targets_column_name, algorithm, train_size, n_estimators=None, seed=None):\n",
        "    ''' Predict collusion applying the ML algorithm '''\n",
        "    print(\"Init Predict!\")\n",
        "    # Datasets to have to simplify the process' time\n",
        "    simplify_process = ['japan', 'italian', 'switzerland_gr_sg', 'american', 'all']\n",
        "\n",
        "    # To assing the dataframes\n",
        "    predictors = df[predictors_column_name]\n",
        "    targets = df[targets_column_name]\n",
        "\n",
        "    # We create the training and test sample, both for predictors and for the objective variable, based on the tender group.\n",
        "    # That is, the bids of a tender either all own to the train group or the test group. They cannot be divided between both groups.\n",
        "    gss = GroupShuffleSplit(n_splits=5, train_size=train_size)\n",
        "    train_index, test_index = next(gss.split(predictors, targets, groups=df['Tender']))\n",
        "    x_train = predictors.loc[train_index]\n",
        "    y_train = targets.loc[train_index]\n",
        "    x_test = predictors.loc[test_index]\n",
        "    y_test = targets.loc[test_index]\n",
        "\n",
        "    #Scale Feature\n",
        "    if algorithm == 'SVC':\n",
        "      scaling = MinMaxScaler(feature_range=(-1,1)).fit(x_train)\n",
        "      x_train = scaling.transform(x_train)\n",
        "      x_test = scaling.transform(x_test)\n",
        "\n",
        "    # Train the model with the selected algorithm\n",
        "    if algorithm == 'ExtraTreesClassifier':\n",
        "        model = ExtraTreesClassifier(n_estimators=n_estimators, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
        "                            max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True,\n",
        "                            oob_score=True, n_jobs=-1, random_state=seed, verbose=0, warm_start=False, class_weight='balanced', ccp_alpha=0.0, max_samples=None)\n",
        "\n",
        "        param_space = {\n",
        "            'n_estimators': (50, 300),\n",
        "            'max_features': ['auto', 'sqrt', 'log2', None],\n",
        "            'min_samples_split': (2, 20),\n",
        "            'min_samples_leaf': (1, 20),\n",
        "        }\n",
        "\n",
        "        classifier = BayesSearchCV(model, param_space, cv=3, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'RandomForestClassifier':\n",
        "        classifier = RandomForestClassifier(n_estimators=n_estimators, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.,\n",
        "                            max_features=None, max_leaf_nodes=None, min_impurity_decrease=0., bootstrap=True,\n",
        "                            oob_score=True, n_jobs=-1, random_state=seed, verbose=0, warm_start=False, class_weight='balanced')\n",
        "    elif algorithm == 'SGDClassifier':\n",
        "        classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=10000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1,\n",
        "                            n_jobs=-1, random_state=seed, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5,\n",
        "                            class_weight=None, warm_start=False, average=False)\n",
        "    elif algorithm == 'AdaBoostClassifier':\n",
        "        model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=1.0, algorithm='SAMME.R', random_state=seed)\n",
        "\n",
        "        param_space = {\n",
        "            'n_estimators': (50, 300),\n",
        "            'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
        "            'algorithm': ['SAMME', 'SAMME.R']\n",
        "        }\n",
        "\n",
        "        classifier = BayesSearchCV(model, param_space, n_iter=50, cv=3, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'GradientBoostingClassifier':\n",
        "        if dataset in simplify_process:\n",
        "            learning_rate = 100\n",
        "            tol = 10\n",
        "            estimators = int(round(n_estimators / 3))\n",
        "        else:\n",
        "            learning_rate = 0.1\n",
        "            tol = 0.0001\n",
        "            estimators = n_estimators\n",
        "        model = GradientBoostingClassifier(loss='deviance', learning_rate=learning_rate, n_estimators=estimators, subsample=1.0, min_samples_split=2, min_samples_leaf=1,\n",
        "                            min_weight_fraction_leaf=0.0, max_depth=None, min_impurity_decrease=0.0, init=None, random_state=seed, max_features=None, verbose=0,\n",
        "                            max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=tol, ccp_alpha=0.0)\n",
        "        param_space = {\n",
        "            'n_estimators': (50, 200),\n",
        "            'learning_rate': (0.01, 0.1, 'log-uniform'),\n",
        "            'max_depth': (3, 10),\n",
        "        }\n",
        "        classifier = BayesSearchCV(n_iter=50, cv=3, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'SVC':\n",
        "        model = SVC(coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200,\n",
        "                            class_weight='balanced', verbose=False, decision_function_shape='ovr', break_ties=False, random_state=seed)\n",
        "        param_space = {\n",
        "            'C': (0.01, 0.1, 1, 10, 100, 1000),\n",
        "            'degree': Integer(1,6),\n",
        "            'gamma' : Real(1e-6, 1e+1, prior='log-uniform'),\n",
        "            'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
        "        }\n",
        "\n",
        "        classifier = BayesSearchCV(model, param_space, n_iter=32, cv=5, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'KNeighborsClassifier':\n",
        "        classifier = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=-1)\n",
        "    elif algorithm == 'MLPClassifier':\n",
        "        model = MLPClassifier(batch_size='auto', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=seed, tol=0.0001, verbose=0, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
        "                        early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
        "        param_space = {\n",
        "            'hidden_layer_sizes' : (240, 120, 70, 35),\n",
        "            'activation': ['tanh'],\n",
        "            'solver': ['adam', 'sgd', 'lbfgs'],\n",
        "            'alpha': [0.0001, 0.01],\n",
        "            'learning_rate': ['constant','adaptive'],\n",
        "        }\n",
        "        classifier = BayesSearchCV(model, param_space, n_iter=32, cv=5, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'GaussianNB':\n",
        "        classifier = GaussianNB(priors=None, var_smoothing=1e-09)\n",
        "    elif algorithm == 'BernoulliNB':\n",
        "        classifier = BernoulliNB(alpha=0.5, binarize=0, fit_prior=True, class_prior=None)\n",
        "    elif algorithm == 'LogisticRegression':\n",
        "        model = LogisticRegression(dual=False, tol=1e-4, fit_intercept=True, intercept_scaling=1.0, class_weight=None,multi_class='auto', verbose=0, warm_start=False, l1_ratio=None, random_state=seed)\n",
        "        param_space = {\n",
        "            'max_iter' : (50, 300),\n",
        "            'C' : (0.01, 0.5),\n",
        "            'solver' : ['newton-cg'],\n",
        "            'penalty' : ('l2', None)\n",
        "        }\n",
        "        classifier = BayesSearchCV(model, param_space, n_iter=50, cv=3, random_state=seed, n_jobs=-1)\n",
        "    elif algorithm == 'GaussianProcessClassifier':\n",
        "        if dataset in simplify_process:\n",
        "            max_iter_predict = 5\n",
        "            n_restarts_optimizer = 2\n",
        "        else:\n",
        "            max_iter_predict = 5000\n",
        "            n_restarts_optimizer = 50\n",
        "        classifier = GaussianProcessClassifier(kernel=None, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=n_restarts_optimizer, max_iter_predict=max_iter_predict, warm_start=False, copy_X_train=True, random_state=seed,\n",
        "                        multi_class='one_vs_rest', n_jobs=-1)\n",
        "\n",
        "    # We build the model for the train group\n",
        "    classifier.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "    # We predict for the values of the test group\n",
        "    predictions = classifier.predict(x_test)\n",
        "    df_predictions = pd.DataFrame(data=predictions, index=y_test.index, columns=['Forecast_collusive_competitor'])\n",
        "\n",
        "    # To calculate the error metrics for the classification binary model\n",
        "    accuracy = accuracy_score(y_test, predictions) * 100\n",
        "    balanced_accuracy = balanced_accuracy_score(y_test, predictions) * 100\n",
        "    precision = precision_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # Ratio of true positives: tp / (tp + fp)\n",
        "    recall = recall_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # Ratio of true positives: tp / (tp + fn)\n",
        "    f1 = f1_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    confusion = confusion_matrix(y_test, predictions, normalize='all') * 100\n",
        "\n",
        "    print(f'Best params for {algorithm}: {classifier.best_params_}')\n",
        "\n",
        "    return accuracy, balanced_accuracy, precision, recall, f1, confusion, y_test, df_predictions\n",
        "\n",
        "def shuffle_tenders(df1, seed):\n",
        "    ''' Shuffle tenders. The reason is that maybe the colluded tenders are concentrated in some parts of the excel (dataframe)'''\n",
        "\n",
        "    df = df1.copy()\n",
        "    df = shuffle(df, random_state=seed).reset_index(drop=True)\n",
        "    df['Tender'] = df['Tender'].astype(str)\n",
        "    reindex_tenders = 1\n",
        "    list_tenders = []\n",
        "    for index, row in df.iterrows():\n",
        "        if not row['Tender'] in list_tenders:\n",
        "            df['Tender'].replace(row['Tender'], reindex_tenders, inplace=True)\n",
        "            reindex_tenders = reindex_tenders + 1\n",
        "            list_tenders.append(row['Tender'])\n",
        "    return df\n",
        "\n",
        "def algorithm_comparison(df, dataset, predictors, targets, algorithms, train_size, repetitions, n_estimators, precision_recall=False, load_data=False, save_data=False, seed=None):\n",
        "    ''' Print table to compare Machine Learning algorithms '''\n",
        "\n",
        "    df = shuffle_tenders(df, seed)\n",
        "\n",
        "    for setting in predictors:\n",
        "        print('')\n",
        "        print('Generating models for ' + setting)\n",
        "        accuracy = defaultdict(list)\n",
        "        balanced_accuracy = defaultdict(list)\n",
        "        false_positive = defaultdict(list)\n",
        "        false_negative = defaultdict(list)\n",
        "        precision = defaultdict(list)\n",
        "        recall = defaultdict(list)\n",
        "        f1 = defaultdict(list)\n",
        "        tenders_test = defaultdict(list)\n",
        "        tenders_predictions = defaultdict(list)\n",
        "\n",
        "        # Create namefile\n",
        "        namefile = dataset + '_ML_algorithms_experimentation_' + setting + '_' + str(repetitions) + 'repetitions'\n",
        "\n",
        "        if load_data == False:\n",
        "            for algorithm in algorithms:\n",
        "                print('Training algorithm ' + algorithm)\n",
        "                df_copy = df.copy()\n",
        "                if algorithm in ['GaussianProcessClassifier', 'GradientBoostingClassifier', 'SVC']:\n",
        "                    loop = int(round(repetitions / 40))\n",
        "\n",
        "                    if dataset == 'all' and algorithm == 'GaussianProcessClassifier':\n",
        "                        # Exception: reduce the datataset to be able to compute this dataset and algorithm\n",
        "                        df_copy = df_copy.sample(frac=0.5).reset_index(drop=True)\n",
        "                else:\n",
        "                    loop = repetitions\n",
        "                loop = 1\n",
        "                for i in range(loop):\n",
        "                    item_accuracy, item_balanced_accuracy, item_precision, item_recall, item_f1, confusion_matrix, item_tenders_test, item_tenders_predictions = \\\n",
        "                                predict_collusion_company(df_copy, dataset, predictors[setting], targets, algorithm, train_size, n_estimators)\n",
        "                    accuracy[algorithm].append(item_accuracy)\n",
        "                    balanced_accuracy[algorithm].append(item_balanced_accuracy)\n",
        "                    if confusion_matrix.shape[1] == 2:\n",
        "                        false_positive[algorithm].append(confusion_matrix[0][1])\n",
        "                        false_negative[algorithm].append(confusion_matrix[1][0])\n",
        "                    else:\n",
        "                        false_positive[algorithm].append(0)\n",
        "                        false_negative[algorithm].append(0)\n",
        "                    precision[algorithm].append(item_precision)\n",
        "                    recall[algorithm].append(item_recall)\n",
        "                    f1[algorithm].append(item_f1)\n",
        "                    tenders_test[algorithm].append(item_tenders_test)\n",
        "                    tenders_predictions[algorithm].append(item_tenders_predictions)\n",
        "\n",
        "            # Save dictionaries to persist the data experimentation\n",
        "            if save_data:\n",
        "                path_namefile = os.path.join(path, namefile + '.pkl')\n",
        "                file = [accuracy, balanced_accuracy, false_positive, false_negative, precision, recall, f1, df, tenders_test, tenders_predictions]\n",
        "                dump(file, path_namefile, compress=6)\n",
        "\n",
        "        else:\n",
        "            # To load data\n",
        "            pkl_file = os.path.join(path, namefile + '.pkl')\n",
        "            [accuracy, balanced_accuracy, false_positive, false_negative, precision, recall, f1, df, tenders_test, tenders_predictions] = load(pkl_file)\n",
        "\n",
        "        for algorithm in algorithms:\n",
        "          mean_balanced_accuracy = np.mean(balanced_accuracy[algorithm])\n",
        "          mean_f1 = np.mean(f1[algorithm])\n",
        "          mean_precision = np.mean(precision[algorithm])\n",
        "          mean_recall = np.mean(recall[algorithm])\n",
        "          mean_accuracy = np.mean(accuracy[algorithm])\n",
        "          mean_false_positive = np.mean(false_positive[algorithm])\n",
        "          mean_false_negative = np.mean(false_negative[algorithm])\n",
        "\n",
        "          # std_balanced_accuracy = np.std(balanced_accuracy[algorithm])\n",
        "          # std_f1 = np.std(f1[algorithm])\n",
        "          # std_precision = np.std(precision[algorithm])\n",
        "          # std_recall = np.std(recall[algorithm])\n",
        "          # std_accuracy = np.std(accuracy[algorithm])\n",
        "          # std_false_positive = np.std(false_positive[algorithm])\n",
        "          # std_false_negative = np.std(false_negative[algorithm])\n",
        "          # Print error metrics\n",
        "          # test_size = 1 - train_size\n",
        "          # print('Algorithm {} with train:test {:,.2f}:{:,.2f}, {} repetitions and {}: mean_accuracy={:,.1f}, mean_FP={:,.1f}, '\n",
        "          #     'mean_FN={:,.1f}, mean_balanced_accuracy={:,.1f}, mean_f1={:,.1f}, median_f1={:,.1f}, mean_precision={:,.1f}, '\n",
        "          #     'median_precision={:,.1f}, mean_recall={:,.1f} and median_recall={:,.1f}'.format(\n",
        "          #     algorithm, train_size, test_size, repetitions, setting, mean_accuracy, mean_false_positive, mean_false_negative,\n",
        "          #     mean_balanced_accuracy, mean_f1, np.median(f1[algorithm]), mean_precision,\n",
        "          #     np.median(precision[algorithm]), mean_recall, np.median(recall[algorithm])))\n",
        "\n",
        "          accuracy_mean[(setting, algorithm)].append(mean_accuracy)\n",
        "          balanced_accuracy_mean[(setting, algorithm)].append(mean_balanced_accuracy)\n",
        "          false_positive_mean[(setting, algorithm)].append(mean_false_positive)\n",
        "          false_negative_mean[(setting, algorithm)].append(mean_false_negative)\n",
        "          precision_mean[(setting, algorithm)].append(mean_precision)\n",
        "          recall_mean[(setting, algorithm)].append(mean_recall)\n",
        "          f1_mean[(setting, algorithm)].append(mean_f1)\n",
        "\n",
        "          # accuracy_std[(setting, algorithm)].append(std_accuracy)\n",
        "          # balanced_accuracy_std[(setting, algorithm)].append(std_balanced_accuracy)\n",
        "          # false_positive_std[(setting, algorithm)].append(std_false_positive)\n",
        "          # false_negative_std[(setting, algorithm)].append(std_false_negative)\n",
        "          # precision_std[(setting, algorithm)].append(std_precision)\n",
        "          # recall_std[(setting, algorithm)].append(std_recall)\n",
        "          # f1_std[(setting, algorithm)].append(std_f1)\n",
        "\n",
        "        # Print curve precision vs recall with iso-F1 lines\n",
        "        # if precision_recall:\n",
        "            # plot_precision_vs_recall(dataset, algorithms, precision, recall, min_f1=0.4, max_f1=0.86, f1_curves=24, min_x_y_lim=0.5, max_x_y_lim=1, namefile=namefile)\n",
        "\n",
        "\n",
        "def get_dataset(dataset):\n",
        "    ''' Get the collusive dataset and their fields to use in the ML algorimths '''\n",
        "\n",
        "    predictors = defaultdict(list)\n",
        "\n",
        "    if dataset == 'brazilian':\n",
        "        df_collusion = pd.read_csv(db_collusion_brazilian, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Date', 'Brazilian State', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Date', 'Brazilian State', 'Winner', 'Number_bids'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'switzerland_gr_sg':\n",
        "        df_collusion = pd.read_csv(db_collusion_switzerland_gr_sg, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Contract_type', 'Date', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'switzerland_ticino':\n",
        "        df_collusion = pd.read_csv(db_collusion_switzerland_ticino, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Consortium', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'italian':\n",
        "        df_collusion = pd.read_csv(db_collusion_italian, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Capital', 'Legal_entity_type', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'american':\n",
        "        df_collusion = pd.read_csv(db_collusion_american, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Bid_value_without_inflation', 'Bid_value_inflation_raw_milk_price_adjusted_bid', 'Date', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'japan':\n",
        "        df_collusion = pd.read_csv(db_collusion_japan, header=0)\n",
        "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Date', 'Winner', 'Number_bids']\n",
        "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
        "\n",
        "    elif dataset == 'all':\n",
        "        df_collusion = pd.read_csv(db_collusion_all, header=0)\n",
        "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids', 'Dataset']\n",
        "\n",
        "    predictors['common+screens'] = predictors['common'] + screens\n",
        "\n",
        "    # Output fields of the datasets to the ML algorithms.\n",
        "    targets = ['Collusive_competitor']\n",
        "\n",
        "    return df_collusion, predictors, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMj_RQbnIABi"
      },
      "outputs": [],
      "source": [
        "# The user selectes the dataset to analyse\n",
        "dataset = 'brazilian'\n",
        "\n",
        "# 1. Get the dataset processed ready to use with the ML algorithms\n",
        "df_collusion, predictors, targets = get_dataset(dataset)\n",
        "\n",
        "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKCXkIEpeNle",
        "outputId": "667fd3c9-d0dc-4414-f96e-41c377186bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating models for all_setting\n",
            "Training algorithm SVC\n",
            "Init Predict!\n",
            "Best params for SVC: OrderedDict([('C', 10.0), ('degree', 6), ('gamma', 0.42243034366179416), ('kernel', 'poly')])\n",
            "\n",
            "Generating models for all_setting+screens\n",
            "Training algorithm SVC\n",
            "Init Predict!\n",
            "Best params for SVC: OrderedDict([('C', 1000.0), ('degree', 1), ('gamma', 0.032521397638469306), ('kernel', 'rbf')])\n",
            "\n",
            "Generating models for common\n",
            "Training algorithm SVC\n",
            "Init Predict!\n",
            "Best params for SVC: OrderedDict([('C', 1000.0), ('degree', 5), ('gamma', 9.01566090470945), ('kernel', 'rbf')])\n",
            "\n",
            "Generating models for common+screens\n",
            "Training algorithm SVC\n",
            "Init Predict!\n",
            "Best params for SVC: OrderedDict([('C', 1000.0), ('degree', 4), ('gamma', 4.0710328666530895), ('kernel', 'poly')])\n"
          ]
        }
      ],
      "source": [
        "# 7. Execute algorithm comparison and print table comparison\n",
        "accuracy_mean = defaultdict(list)\n",
        "balanced_accuracy_mean = defaultdict(list)\n",
        "false_positive_mean = defaultdict(list)\n",
        "false_negative_mean = defaultdict(list)\n",
        "precision_mean = defaultdict(list)\n",
        "recall_mean = defaultdict(list)\n",
        "f1_mean = defaultdict(list)\n",
        "\n",
        "accuracy_std = defaultdict(list)\n",
        "balanced_accuracy_std = defaultdict(list)\n",
        "false_positive_std = defaultdict(list)\n",
        "false_negative_std = defaultdict(list)\n",
        "precision_std = defaultdict(list)\n",
        "recall_std = defaultdict(list)\n",
        "f1_std = defaultdict(list)\n",
        "\n",
        "algorithm_comparison(df_collusion, dataset, predictors, targets, ml_algorithms, train_size, repetitions, n_estimators, precision_recall, load_data, save_data, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNs8wVzEO-b6"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Generating models for all_setting\n",
        "Training algorithm GradientBoostingClassifier\n",
        "Init Predict!\n",
        "Best params for GradientBoostingClassifier: OrderedDict([('learning_rate', 0.09991742700717861), ('max_depth', 4), ('n_estimators', 170)])\n",
        "Training algorithm ExtraTreesClassifier\n",
        "Init Predict!\n",
        "Best params for ExtraTreesClassifier: OrderedDict([('max_features', None), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 300)])\n",
        "Training algorithm AdaBoostClassifier\n",
        "Init Predict!\n",
        "Best params for AdaBoostClassifier: OrderedDict([('algorithm', 'SAMME'), ('learning_rate', 1.0), ('n_estimators', 300)])\n",
        "\n",
        "Generating models for all_setting+screens\n",
        "Training algorithm GradientBoostingClassifier\n",
        "Init Predict!\n",
        "Best params for GradientBoostingClassifier: OrderedDict([('learning_rate', 0.09126660627584679), ('max_depth', 10), ('n_estimators', 111)])\n",
        "Training algorithm ExtraTreesClassifier\n",
        "Init Predict!\n",
        "Best params for ExtraTreesClassifier: OrderedDict([('max_features', None), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 300)])\n",
        "Training algorithm AdaBoostClassifier\n",
        "Init Predict!\n",
        "Best params for AdaBoostClassifier: OrderedDict([('algorithm', 'SAMME.R'), ('learning_rate', 0.6104443013206723), ('n_estimators', 290)])\n",
        "\n",
        "Generating models for common\n",
        "Training algorithm GradientBoostingClassifier\n",
        "Init Predict!\n",
        "Best params for GradientBoostingClassifier: OrderedDict([('learning_rate', 0.1), ('max_depth', 4), ('n_estimators', 181)])\n",
        "Training algorithm ExtraTreesClassifier\n",
        "Init Predict!\n",
        "Best params for ExtraTreesClassifier: OrderedDict([('max_features', None), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 65)])\n",
        "Training algorithm AdaBoostClassifier\n",
        "Init Predict!\n",
        "Best params for AdaBoostClassifier: OrderedDict([('algorithm', 'SAMME.R'), ('learning_rate', 0.2850791538944987), ('n_estimators', 191)])\n",
        "\n",
        "Generating models for common+screens\n",
        "Training algorithm GradientBoostingClassifier\n",
        "Init Predict!\n",
        "Best params for GradientBoostingClassifier: OrderedDict([('learning_rate', 0.06881923355419943), ('max_depth', 3), ('n_estimators', 126)])\n",
        "Training algorithm ExtraTreesClassifier\n",
        "Init Predict!\n",
        "Best params for ExtraTreesClassifier: OrderedDict([('max_features', None), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 300)])\n",
        "Training algorithm AdaBoostClassifier\n",
        "Init Predict!\n",
        "Best params for AdaBoostClassifier: OrderedDict([('algorithm', 'SAMME'), ('learning_rate', 1.0), ('n_estimators', 130)])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_MHAtLemWe"
      },
      "outputs": [],
      "source": [
        "keys = accuracy_mean.keys()\n",
        "\n",
        "for key in keys:\n",
        "  accuracy_std[key] = np.std(accuracy_mean[key])\n",
        "  balanced_accuracy_std[key] = np.std(balanced_accuracy_mean[key])\n",
        "  false_positive_std[key] = np.std(false_positive_mean[key])\n",
        "  false_negative_std[key] = np.std(false_negative_mean[key])\n",
        "  precision_std[key] = np.std(precision_mean[key])\n",
        "  recall_std[key] = np.std(recall_mean[key])\n",
        "  f1_std[key] = np.std(f1_mean[key])\n",
        "\n",
        "  accuracy_mean[key] = np.mean(accuracy_mean[key])\n",
        "  balanced_accuracy_mean[key] = np.mean(balanced_accuracy_mean[key])\n",
        "  false_positive_mean[key] = np.mean(false_positive_mean[key])\n",
        "  false_negative_mean[key] = np.mean(false_negative_mean[key])\n",
        "  precision_mean[key] = np.mean(precision_mean[key])\n",
        "  recall_mean[key] = np.mean(recall_mean[key])\n",
        "  f1_mean[key] = np.mean(f1_mean[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSP4eq04QiYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af82844-c7f9-4e3c-a976-b027d8a3cccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"accuracy_mean\": {\n",
            "        \"all_setting:SVC\": 84.12698412698413,\n",
            "        \"all_setting+screens:SVC\": 82.20338983050848,\n",
            "        \"common:SVC\": 78.37837837837837,\n",
            "        \"common+screens:SVC\": 83.33333333333334\n",
            "    },\n",
            "    \"balanced_accuracy_mean\": {\n",
            "        \"all_setting:SVC\": 64.20454545454545,\n",
            "        \"all_setting+screens:SVC\": 78.95153626860944,\n",
            "        \"common:SVC\": 74.56073338426279,\n",
            "        \"common+screens:SVC\": 90.07633587786259\n",
            "    },\n",
            "    \"false_positive_mean\": {\n",
            "        \"all_setting:SVC\": 7.936507936507936,\n",
            "        \"all_setting+screens:SVC\": 6.779661016949152,\n",
            "        \"common:SVC\": 10.81081081081081,\n",
            "        \"common+screens:SVC\": 16.666666666666664\n",
            "    },\n",
            "    \"false_negative_mean\": {\n",
            "        \"all_setting:SVC\": 7.936507936507936,\n",
            "        \"all_setting+screens:SVC\": 11.016949152542372,\n",
            "        \"common:SVC\": 10.81081081081081,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"precision_mean\": {\n",
            "        \"all_setting:SVC\": 37.5,\n",
            "        \"all_setting+screens:SVC\": 77.77777777777779,\n",
            "        \"common:SVC\": 64.70588235294117,\n",
            "        \"common+screens:SVC\": 49.01960784313725\n",
            "    },\n",
            "    \"recall_mean\": {\n",
            "        \"all_setting:SVC\": 37.5,\n",
            "        \"all_setting+screens:SVC\": 68.29268292682927,\n",
            "        \"common:SVC\": 64.70588235294117,\n",
            "        \"common+screens:SVC\": 100.0\n",
            "    },\n",
            "    \"f1_mean\": {\n",
            "        \"all_setting:SVC\": 37.5,\n",
            "        \"all_setting+screens:SVC\": 72.72727272727273,\n",
            "        \"common:SVC\": 64.70588235294117,\n",
            "        \"common+screens:SVC\": 65.78947368421052\n",
            "    },\n",
            "    \"accuracy_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"balanced_accuracy_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"false_positive_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"false_negative_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"precision_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"recall_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    },\n",
            "    \"f1_std\": {\n",
            "        \"all_setting:SVC\": 0.0,\n",
            "        \"all_setting+screens:SVC\": 0.0,\n",
            "        \"common:SVC\": 0.0,\n",
            "        \"common+screens:SVC\": 0.0\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "results_json = defaultdict(dict)\n",
        "\n",
        "results_json['accuracy_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in accuracy_mean.items()}\n",
        "results_json['balanced_accuracy_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in balanced_accuracy_mean.items()}\n",
        "results_json['false_positive_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in false_positive_mean.items()}\n",
        "results_json['false_negative_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in false_negative_mean.items()}\n",
        "results_json['precision_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in precision_mean.items()}\n",
        "results_json['recall_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in recall_mean.items()}\n",
        "results_json['f1_mean'] = {\":\".join(map(str, chave)): valor for chave, valor in f1_mean.items()}\n",
        "\n",
        "results_json['accuracy_std'] = {\":\".join(map(str, chave)): valor for chave, valor in accuracy_std.items()}\n",
        "results_json['balanced_accuracy_std'] = {\":\".join(map(str, chave)): valor for chave, valor in balanced_accuracy_std.items()}\n",
        "results_json['false_positive_std'] = {\":\".join(map(str, chave)): valor for chave, valor in false_positive_std.items()}\n",
        "results_json['false_negative_std'] = {\":\".join(map(str, chave)): valor for chave, valor in false_negative_std.items()}\n",
        "results_json['precision_std'] = {\":\".join(map(str, chave)): valor for chave, valor in precision_std.items()}\n",
        "results_json['recall_std'] = {\":\".join(map(str, chave)): valor for chave, valor in recall_std.items()}\n",
        "results_json['f1_std'] = {\":\".join(map(str, chave)): valor for chave, valor in f1_std.items()}\n",
        "\n",
        "import json\n",
        "\n",
        "json_data = json.dumps(results_json, indent=4)\n",
        "\n",
        "print(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "843ndhBUrPMH"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"accuracy_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 70.29702970297029,\n",
        "        \"all_setting:ExtraTreesClassifier\": 91.0828025477707,\n",
        "        \"all_setting:AdaBoostClassifier\": 84.9624060150376,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 97.91666666666666,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 93.71069182389937,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 85.41666666666666,\n",
        "        \"common:GradientBoostingClassifier\": 88.7218045112782,\n",
        "        \"common:ExtraTreesClassifier\": 90.08264462809917,\n",
        "        \"common:AdaBoostClassifier\": 92.90780141843972,\n",
        "        \"common+screens:GradientBoostingClassifier\": 97.95918367346938,\n",
        "        \"common+screens:ExtraTreesClassifier\": 86.22754491017965,\n",
        "        \"common+screens:AdaBoostClassifier\": 90.19607843137256\n",
        "    },\n",
        "    \"balanced_accuracy_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 65.9090909090909,\n",
        "        \"all_setting:ExtraTreesClassifier\": 94.85294117647058,\n",
        "        \"all_setting:AdaBoostClassifier\": 58.7474645030426,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 98.79032258064517,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 75.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 85.87719298245614,\n",
        "        \"common:GradientBoostingClassifier\": 82.26950354609929,\n",
        "        \"common:ExtraTreesClassifier\": 83.33333333333333,\n",
        "        \"common:AdaBoostClassifier\": 83.87096774193547,\n",
        "        \"common+screens:GradientBoostingClassifier\": 97.01963534361852,\n",
        "        \"common+screens:ExtraTreesClassifier\": 80.33681214421253,\n",
        "        \"common+screens:AdaBoostClassifier\": 73.21428571428572\n",
        "    },\n",
        "    \"false_positive_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 8.9171974522293,\n",
        "        \"all_setting:AdaBoostClassifier\": 5.263157894736842,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 2.083333333333333,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 11.805555555555555,\n",
        "        \"common:GradientBoostingClassifier\": 1.5037593984962405,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 1.3605442176870748,\n",
        "        \"common+screens:ExtraTreesClassifier\": 8.383233532934131,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"false_negative_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 29.7029702970297,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 9.774436090225564,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 6.289308176100629,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 2.7777777777777777,\n",
        "        \"common:GradientBoostingClassifier\": 9.774436090225564,\n",
        "        \"common:ExtraTreesClassifier\": 9.917355371900827,\n",
        "        \"common:AdaBoostClassifier\": 7.092198581560284,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.6802721088435374,\n",
        "        \"common+screens:ExtraTreesClassifier\": 5.389221556886228,\n",
        "        \"common+screens:AdaBoostClassifier\": 9.803921568627452\n",
        "    },\n",
        "    \"precision_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 100.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 60.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 36.36363636363637,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 86.95652173913044,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 100.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 60.46511627906976,\n",
        "        \"common:GradientBoostingClassifier\": 92.85714285714286,\n",
        "        \"common:ExtraTreesClassifier\": 100.0,\n",
        "        \"common:AdaBoostClassifier\": 100.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 91.66666666666666,\n",
        "        \"common+screens:ExtraTreesClassifier\": 61.111111111111114,\n",
        "        \"common+screens:AdaBoostClassifier\": 100.0\n",
        "    },\n",
        "    \"recall_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 31.818181818181817,\n",
        "        \"all_setting:ExtraTreesClassifier\": 100.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 23.52941176470588,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 100.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 50.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 86.66666666666667,\n",
        "        \"common:GradientBoostingClassifier\": 66.66666666666666,\n",
        "        \"common:ExtraTreesClassifier\": 66.66666666666666,\n",
        "        \"common:AdaBoostClassifier\": 67.74193548387096,\n",
        "        \"common+screens:GradientBoostingClassifier\": 95.65217391304348,\n",
        "        \"common+screens:ExtraTreesClassifier\": 70.96774193548387,\n",
        "        \"common+screens:AdaBoostClassifier\": 46.42857142857143\n",
        "    },\n",
        "    \"f1_mean\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 48.275862068965516,\n",
        "        \"all_setting:ExtraTreesClassifier\": 74.99999999999999,\n",
        "        \"all_setting:AdaBoostClassifier\": 28.57142857142857,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 93.02325581395348,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 66.66666666666666,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 71.23287671232877,\n",
        "        \"common:GradientBoostingClassifier\": 77.61194029850746,\n",
        "        \"common:ExtraTreesClassifier\": 80.0,\n",
        "        \"common:AdaBoostClassifier\": 80.76923076923077,\n",
        "        \"common+screens:GradientBoostingClassifier\": 93.61702127659574,\n",
        "        \"common+screens:ExtraTreesClassifier\": 65.67164179104478,\n",
        "        \"common+screens:AdaBoostClassifier\": 63.41463414634146\n",
        "    },\n",
        "    \"accuracy_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"balanced_accuracy_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"false_positive_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"false_negative_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"precision_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"recall_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    },\n",
        "    \"f1_std\": {\n",
        "        \"all_setting:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting:AdaBoostClassifier\": 0.0,\n",
        "        \"all_setting+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"all_setting+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"all_setting+screens:AdaBoostClassifier\": 0.0,\n",
        "        \"common:GradientBoostingClassifier\": 0.0,\n",
        "        \"common:ExtraTreesClassifier\": 0.0,\n",
        "        \"common:AdaBoostClassifier\": 0.0,\n",
        "        \"common+screens:GradientBoostingClassifier\": 0.0,\n",
        "        \"common+screens:ExtraTreesClassifier\": 0.0,\n",
        "        \"common+screens:AdaBoostClassifier\": 0.0\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}